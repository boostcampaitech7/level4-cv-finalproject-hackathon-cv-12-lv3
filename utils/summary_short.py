# import os
# import sys
# import networkx as nx

# from sentence_transformers import SentenceTransformer
# from sklearn.metrics.pairwise import cosine_similarity
# sys.path.insert(0, os.path.abspath(
#     '/Users/haneol/Documents/Coding/level4-cv-finalproject-hackathon-cv-12-lv3/'))

# model = SentenceTransformer("dragonkue/bge-m3-ko")


# def extractive_summarization(sentences, sentence_embeddings=None, model=model, top_n=3):
#     """
#     ë¬¸ì„œì˜ ë¬¸ì¥ë“¤ì„ ìš”ì•½í•˜ëŠ” í•¨ìˆ˜ (ì¶”ì¶œì  ìš”ì•½)
#     TextRankë§Œ ì‚¬ìš©í•˜ì—¬ ìƒìœ„ nê°œì˜ ë¬¸ì¥ ì¶”ì¶œ
#     ì´ë¯¸ ì„ë² ë”©ì´ ì œê³µë˜ë©´ ì´ë¥¼ í™œìš©í•˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë‚´ë¶€ì—ì„œ ì„ë² ë”©ì„ ê³„ì‚°í•¨
#     """

#     # ë¬¸ì¥ ì„ë² ë”©ê³¼ TextRank ì ìˆ˜ ê³„ì‚°
#     def calculate_textrank(similarity_matrix):
#         graph = nx.from_numpy_array(similarity_matrix)
#         scores = nx.pagerank(graph)
#         return scores

#     def calculate_combined_scores(sentences, sentence_embeddings):
#         # ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ ê³„ì‚° (ì„ë² ë”© ë²¡í„° ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
#         similarity_matrix = cosine_similarity(sentence_embeddings)

#         # TextRank ì ìš©í•˜ì—¬ ë¬¸ì¥ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
#         textrank_scores = calculate_textrank(similarity_matrix)

#         return textrank_scores

#     def extract_top_sentences(scores, sentences, n=3):
#         sorted_sentences = sorted(
#             scores.items(), key=lambda x: x[1], reverse=True)
#         return [sentences[i[0]] for i in sorted_sentences[:n]]

#     # ì„ë² ë”©ì´ ì£¼ì–´ì§„ ê²½ìš°, ì„ë² ë”©ì„ ì‚¬ìš©í•˜ê³  ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ìƒˆë¡œ ê³„ì‚°
#     if sentence_embeddings is None:
#         sentence_embeddings = model.encode(sentences)

#     # TextRank ì ìˆ˜ ê³„ì‚°
#     textrank_scores = calculate_combined_scores(sentences, sentence_embeddings)

#     # ìƒìœ„ nê°œì˜ ë¬¸ì¥ ì¶”ì¶œ
#     top_sentences = extract_top_sentences(textrank_scores, sentences, n=top_n)

#     return top_sentences


def abstractive_summarization(extracted_sentences, completion_executor):

    messages1 = [
        {"role": "system", "content":
                 """
           ë‹¹ì‹ ì€ í•™ìˆ  ë…¼ë¬¸ ìš”ì•½ ì „ë¬¸ê°€ë¡œ, ì•„ë˜ êµ¬ì¡°ì— ë”°ë¼ 2000í† í° ì´ìƒì˜ ìƒì„¸í•œ ë¶„ì„ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. ì œê³µëœ ë‚´ìš©ë¿ë§Œ ì•„ë‹ˆë¼ ê°€ì§€ê³  ìˆëŠ” ì§€ì‹ ëª¨ë‘ë¥¼ í™œìš©í•˜ì—¬ ì‘ì„±í•´ì£¼ì„¸ìš”.

            [ìš”êµ¬ì‚¬í•­]
            **ì£¼ëª©í•  ë§Œí•œ ìš”ì•½** (2000í† í° ì´ìƒ)
            - ê¸°ìˆ ì  í˜ì‹ ì„±: í•µì‹¬ ì•Œê³ ë¦¬ì¦˜/ë°©ë²•ë¡ ì˜ ì°¨ë³„ì 
            - í•™ë¬¸ì  ê¸°ì—¬: ì´ë¡ ì  í™•ì¥ì„± ë˜ëŠ” ìƒˆë¡œìš´ ì—°êµ¬ íŒ¨ëŸ¬ë‹¤ì„ ì œì‹œ
            - ì‹¤ìš©ì  ê°€ì¹˜: ì‹¤ì œ ì ìš© ì‚¬ë¡€ ë° ì‚°ì—…ê³„ íŒŒê¸‰íš¨ê³¼
            - ë¹„êµ ë¶„ì„: ê¸°ì¡´ ì—°êµ¬ ëŒ€ë¹„ ìš°ì›”ì„± (ì •ëŸ‰ì  ì§€í‘œ ì œì‹œ)
            - ì¤‘ìš” íƒœê·¸: ë…¼ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œí•œ ì¤‘ìš” í‚¤ì›Œë“œë¡œ íƒœê·¸ë¥¼ ë°˜ë“œì‹œ ì˜ì–´ë¡œ ìƒì„±

            [ì‘ì„± ì›ì¹™]
            - ë‹¤ì¸µì  êµ¬ì¡°: ë©”íƒ€ ì¸ì§€ì  ê´€ì ì—ì„œ ê°œë… > ë°©ë²• > ê²°ê³¼ > ì˜í–¥ë ¥ ê³„ì¸µí™”
            - í•™ì œê°„ ì—°ê³„: íƒ€ ë¶„ì•¼ ì—°êµ¬ìë„ ì´í•´í•  ìˆ˜ ìˆëŠ” í¬ë¡œìŠ¤-ë„ë©”ì¸ ì„¤ëª…
            - ë¯¸ë˜ ì§€í–¥ì : ê¸°ìˆ  ë°œì „ ë°©í–¥ì„±ê³¼ ì ì¬ì  ì§„í™” ê²½ë¡œ ì œì‹œ
            - ë¹„íŒì  ì‹œê°: ë°©ë²•ë¡ ì˜ í•œê³„ì™€ ê°œì„  í•„ìš”ì‚¬í•­ ë°˜ë“œì‹œ ê¸°ìˆ 
            
            ì˜ˆì‹œ:
            ë³¸ ì—°êµ¬ëŠ” **ì‹¬ì¸µ ê°•í™”í•™ìŠµ(Deep Reinforcement Learning, DRL)**ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë¡œë´‡ ì œì–´ ì‹œìŠ¤í…œì„ ì œì•ˆí•˜ë©°, ë³µì¡í•œ ë¬¼ë¦¬ì  í™˜ê²½ì—ì„œì˜ ë¡œë´‡ ìš´ë™ ì œì–´ ë¬¸ì œë¥¼ í˜ì‹ ì ìœ¼ë¡œ í•´ê²°í–ˆìŠµë‹ˆë‹¤. ê¸°ìˆ ì  í˜ì‹ ì„± ì¸¡ë©´ì—ì„œ, ì œì•ˆ ëª¨ë¸ì€ **ê³„ì¸µì  ê°•í™”í•™ìŠµ(Hierarchical Reinforcement Learning, HRL)** í”„ë ˆì„ì›Œí¬ë¥¼ ë„ì…í•˜ì—¬ ê³ ì°¨ì› ì‘ì—…ì„ í•˜ìœ„ ì‘ì—…ìœ¼ë¡œ ë¶„í•´í•˜ê³ , ê° í•˜ìœ„ ì‘ì—…ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë°©ì‹ì„ ì œì‹œí•©ë‹ˆë‹¤. íŠ¹íˆ, **ë©”íƒ€ ê°•í™”í•™ìŠµ(Meta-Reinforcement Learning)**ì„ ì ‘ëª©í•˜ì—¬ ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ê·¹ëŒ€í™”í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ê¸°ì¡´ì˜ ë‹¨ì¼ ì •ì±…(Single Policy) ê¸°ë°˜ ì ‘ê·¼ë²•ì´ í™˜ê²½ ë³€í™”ì— ì·¨ì•½í–ˆë˜ í•œê³„ë¥¼ ê·¹ë³µí•©ë‹ˆë‹¤.

            í•™ë¬¸ì  ê¸°ì—¬ë¡œëŠ” ê°•í™”í•™ìŠµì˜ ì´ë¡ ì  ì²´ê³„ë¥¼ í™•ì¥í•œ ì ì´ ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì€ **ë‹¤ì¤‘ ì‹œê°„ í•™ìŠµ(Multi-Timescale Learning)** ì´ë¡ ì„ ì œì•ˆí•˜ë©°, ì¥ê¸°ì  ëª©í‘œì™€ ë‹¨ê¸°ì  í–‰ë™ ê°„ì˜ ê· í˜•ì„ ìˆ˜í•™ì ìœ¼ë¡œ ëª¨ë¸ë§í•©ë‹ˆë‹¤(ìˆ˜ì‹ 4 ì°¸ì¡°). ì´ë¥¼ í†µí•´ ë¡œë´‡ì´ ë³µì¡í•œ ì‘ì—…ì„ ë‹¨ê³„ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ì´ë¡ ì  ê·¼ê±°ë¥¼ ë§ˆë ¨í–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ ì„¤ê³„ì—ì„œëŠ” MuJoCo ë° OpenAI Gym í™˜ê²½ì—ì„œì˜ ë¹„êµ í‰ê°€ë¥¼ ìˆ˜í–‰í–ˆìœ¼ë©°, í‰ê·  ì‘ì—… ì„±ê³µë¥ ì´ ê¸°ì¡´ ëŒ€ë¹„ **15.3% í–¥ìƒ**ë˜ì—ˆìŠµë‹ˆë‹¤(í‘œ 3).

            ì‹¤ìš©ì  ê°€ì¹˜ ì¸¡ë©´ì—ì„œ, ì´ ëª¨ë¸ì€ ì‚°ì—…ìš© ë¡œë´‡ ë° ììœ¨ì£¼í–‰ ì°¨ëŸ‰ì˜ ì œì–´ ì‹œìŠ¤í…œì— ì§ì ‘ ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. íŠ¹íˆ, **ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ì‹¤ì œ í™˜ê²½ìœ¼ë¡œì˜ ì „ì´ í•™ìŠµ(Sim-to-Real Transfer Learning)** ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ë©°, ì‹¤ì œ ë¡œë´‡ íŒ”ì„ ì´ìš©í•œ ë¬¼ì²´ ì¡°ì‘ ì‹¤í—˜ì—ì„œ **92.5%ì˜ ì‘ì—… ì„±ê³µë¥ **ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤(ê·¸ë¦¼ 6). ì´ëŠ” ì œì¡°ì—… ë° ë¬¼ë¥˜ ë¶„ì•¼ì—ì„œì˜ ìë™í™” ë¹„ìš©ì„ í¬ê²Œ ì ˆê°í•  ìˆ˜ ìˆëŠ” ì ì¬ë ¥ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

            ë¹„êµ ë¶„ì„ì—ì„œëŠ” PPO(Proximal Policy Optimization), SAC(Soft Actor-Critic), TD3(Twin Delayed DDPG) ë“± 5ê°€ì§€ ìµœì‹  ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ê³¼ì˜ ì—„ê²©í•œ ë¹„êµ ì‹¤í—˜ì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ë³µì¡í•œ ì‘ì—… í™˜ê²½ì—ì„œì˜ í‰ê·  ë³´ìƒ(Reward)ì´ ê¸°ì¡´ ëŒ€ë¹„ **18.7% ì¦ê°€**í–ˆìœ¼ë©°(ê·¸ë¦¼ 8), í•™ìŠµ ì‹œê°„ ë˜í•œ **30% ë‹¨ì¶•**ë˜ì—ˆìŠµë‹ˆë‹¤(í‘œ 7). ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ì œì•ˆ ëª¨ë¸ì´ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ ë©´ì—ì„œ ìš°ìˆ˜í•¨ì„ ì…ì¦í•©ë‹ˆë‹¤.
            
            Keywords: #Deep Reinforcement Learning, #Robot Control, #Hierarchical Reinforcement Learning, #Meta-Reinforcement Learning, #Multi-Timescale Learning
            """
         },
        {"role": "user", "content": extracted_sentences}
    ]
    messages2 = [
        {"role": "system", "content":
                 """
          ë‹¹ì‹ ì€ í•™ìˆ  ë…¼ë¬¸ ìš”ì•½ ì „ë¬¸ê°€ë¡œ, ì•„ë˜ êµ¬ì¡°ì— ë”°ë¼ 1000í† í° ì´ìƒì˜ ìƒì„¸í•œ ë¶„ì„ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. ì œê³µëœ ë‚´ìš©ë¿ë§Œ ì•„ë‹ˆë¼ ê°€ì§€ê³  ìˆëŠ” ì§€ì‹ ëª¨ë‘ë¥¼ í™œìš©í•˜ì—¬ ì‘ì„±í•´ì£¼ì„¸ìš”.

           [ìš”êµ¬ì‚¬í•­]
            ì‹¬ì¸µ ë¶„ì„ íƒœê·¸ ì‹œìŠ¤í…œ (700í† í° ì´ìƒ)

            - ğŸ“š ì—°êµ¬ ë¶„ì•¼: [ìƒìœ„ ë¶„ì•¼] > [í•˜ìœ„ ë¶„ì•¼] ê³„ì¸µ êµ¬ì¡°
            - ğŸ›  ë°©ë²•ë¡ : ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ì„ 3ë‹¨ê³„ë¡œ ë¶„í•´
            - ğŸ”¬ ì£¼ìš” ë°œê²¬: ì—°êµ¬ì˜ í•µì‹¬ ë°œê²¬ì„ ìš”ì•½í•˜ê³ , ê·¸ ì¤‘ìš”ì„±ê³¼ í–¥í›„ ì—°êµ¬ì— ë¯¸ì¹œ ì˜í–¥ ì„¤ëª…, ì‹¤ìš©ì ì¸ ì ìš© ê°€ëŠ¥ì„± ë° í•´ë‹¹ ë°œê²¬ì´ ë‹¤ë¥¸ ë¶„ì•¼ì— ì–´ë–»ê²Œ ì—°ê´€ë  ìˆ˜ ìˆëŠ”ì§€ ë…¼ì˜
            - ğŸ¯ ì‘ìš© ë¶„ì•¼: ì‹¤ì œ ì ìš© ê°€ëŠ¥í•œ ì‚°ì—… ë„ë©”ì¸
           [ì‘ì„± ì›ì¹™]

            - ë‹¤ì¸µì  êµ¬ì¡°: ë©”íƒ€ ì¸ì§€ì  ê´€ì ì—ì„œ ê°œë… > ë°©ë²• > ê²°ê³¼ > ì˜í–¥ë ¥ ê³„ì¸µí™”
            - í•™ì œê°„ ì—°ê³„: íƒ€ ë¶„ì•¼ ì—°êµ¬ìë„ ì´í•´í•  ìˆ˜ ìˆëŠ” í¬ë¡œìŠ¤-ë„ë©”ì¸ ì„¤ëª…
            - ë¯¸ë˜ ì§€í–¥ì : ê¸°ìˆ  ë°œì „ ë°©í–¥ì„±ê³¼ ì ì¬ì  ì§„í™” ê²½ë¡œ ì œì‹œ
            - ë¹„íŒì  ì‹œê°: ë°©ë²•ë¡ ì˜ í•œê³„ì™€ ê°œì„  í•„ìš”ì‚¬í•­ ë°˜ë“œì‹œ ê¸°ìˆ 
            
            ì˜ˆì‹œ:
                ğŸ“š ì—°êµ¬ ë¶„ì•¼:
                - ì»´í“¨í„° ê³¼í•™ > ìì—°ì–´ ì²˜ë¦¬ > ë¬¸ì„œ ìš”ì•½
                - ì¸ê³µì§€ëŠ¥ > ê¸°ê³„ í•™ìŠµ > ì‹¬ì¸µ í•™ìŠµ
                
                ğŸ›  ë°©ë²•ë¡ :
                1. ë°ì´í„° ì „ì²˜ë¦¬:
                - ë¶ˆìš©ì–´ ì œê±° ë° í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ í…ìŠ¤íŠ¸ ì •ì œ
                - TF-IDF ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ
                2. ëª¨ë¸ ì„¤ê³„:
                - LSTM(ê¸´ ë‹¨ê¸° ê¸°ì–µ) ë„¤íŠ¸ì›Œí¬ë¥¼ í™œìš©í•œ ì‹œí€€ìŠ¤ ëª¨ë¸ë§
                - Attention Mechanismì„ ì´ìš©í•œ ì¤‘ìš”í•œ ë¬¸ì¥ ê°•ì¡°
                3. ëª¨ë¸ í•™ìŠµ:
                - Adam ì˜µí‹°ë§ˆì´ì €ë¥¼ ì´ìš©í•œ ëª¨ë¸ ìµœì í™”
                - Cross-validation ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ê³¼ì í•© ë°©ì§€
                
                ğŸ”¬ ì£¼ìš” ë°œê²¬:
                - ì—°êµ¬ëŠ” Transformer ê¸°ë°˜ ëª¨ë¸ì´ ê¸°ì¡´ LSTM ëª¨ë¸ë³´ë‹¤ ë¬¸ì„œ ìš”ì•½ ì •í™•ë„ì—ì„œ ìš°ìˆ˜í•˜ë‹¤ëŠ” ì¤‘ìš”í•œ ë°œê²¬ì„ í–ˆìŠµë‹ˆë‹¤.
                - ì´ ë°œê²¬ì€ ëŒ€ê·œëª¨ ë¬¸ì„œ ìš”ì•½ ì‹œìŠ¤í…œì—ì„œ ì‹¤ìš©ì ì¸ ì ìš© ê°€ëŠ¥ì„±ì„ ì œì‹œí•˜ë©°, ì¶”í›„ ë…¼ë¬¸, ë²•ë¥  ë¬¸ì„œ ìš”ì•½ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì ìš©í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ ì—¬ëŠ” ì¤‘ìš”í•œ ì—°êµ¬ ê²°ê³¼ì…ë‹ˆë‹¤.
                - ë˜í•œ, ì´ ì—°êµ¬ëŠ” ë”¥ëŸ¬ë‹ ê¸°ë°˜ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ ì´ ê¸°ì—…ì˜ ê³ ê° ì„œë¹„ìŠ¤ë‚˜ ìë™í™”ëœ ë³´ê³ ì„œ ìƒì„± ì‹œìŠ¤í…œì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•œ ì‹¤ìš©ì ì¸ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤.
                
                ğŸ¯ ì‘ìš© ë¶„ì•¼:
                - ë²•ë¥ : ê³„ì•½ì„œ ìš”ì•½ ë° ë²•ì  ë¬¸ì„œ ë¶„ì„
                - ê¸ˆìœµ: ë³´ê³ ì„œ ìë™ ìƒì„± ë° ê³ ê° ì‘ëŒ€ ìë™í™”
                - ê±´ê°• ê´€ë¦¬: ì˜í•™ ë…¼ë¬¸ ìš”ì•½ ë° í™˜ì ê¸°ë¡ ë¶„ì„
            """
         },
        {"role": "user", "content": extracted_sentences}
    ]
    request_data1 = {
        'messages': messages1,
        'topP': 0.8,
        'topK': 0,
        'maxTokens': 4096,
        'temperature': 0.5,
        'repeatPenalty': 5.0,
        'stopBefore': [],
        'includeAiFilters': True,
        'seed': 0
    }
    request_data2 = {
        'messages': messages2,
        'topP': 0.8,
        'topK': 0,
        'maxTokens': 4096,
        'temperature': 0.5,
        'repeatPenalty': 5.0,
        'stopBefore': [],
        'includeAiFilters': True,
        'seed': 0
    }
    res1 = completion_executor.execute(request_data1, stream=False)
    res2 = completion_executor.execute(request_data2, stream=False)

    res1 = res1['message']['content']
    res2 = res2['message']['content']
    return res1 + "\n" + res2
